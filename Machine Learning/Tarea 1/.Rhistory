abline(h = 0, v = 0, col = "gray60")
# Ref: https://www.r-bloggers.com/logistic-regression-with-r-step-by-step-implementation-part-2/
# Cost Function
#
CostFunction <- function(parameters, X, Y) {
n <- nrow(X)
# function to apply (%*% Matrix multiplication)
g <- Sigmoid(X %*% parameters)
J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
return(J)
}
#Load data
data <- read.csv("data/4_1_data.csv")
#Create plot
plot(data$score.1, data$score.2, col = as.factor(data$label), xlab = "Score-1", ylab = "Score-2")
#Predictor variables
X <- as.matrix(data[, c(1,2)])
#Add ones to X in the first column (matrix multiplication x b)
X <- cbind(rep(1, nrow(X)), X)
#Response variable
Y <- as.matrix(data$label)
#Intial parameters
initial_parameters <- rep(0, ncol(X))
#Cost at inital parameters
CostFunction(initial_parameters, X, Y)
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
# How to use
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
print("Probability of admission for student:")
print(prob_new_student <- Sigmoid(t(new_student) %*% parameters))
if (!require("gradDescent")) install.packages("gradDescent")
# load
library("gradDescent")
# We want to minimize the cost function. Then derivate this funcion
TestGradientDescent2 <- function(iterations = 1200, learning_rate = 0.25, the_data) {
# label in the last column in dataSet
model <- gradDescentR.learn(dataSet = the_data, featureScaling = TRUE, scalingMethod = "VARIANCE",
learningMethod = "GD", control = list(alpha = learning_rate, maxIter = iterations),
seed = 1234)
model
}
# How to use
TestGradientDescent2(the_data = data)
# Now, the exercises. Use training and test set, change the value of alpha...
# Install if not installed
if (!require("testthat")) install.packages("testthat")
# load
library(testthat)
test_that("Test TestGradientDescent",{
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0135)
# Fail, test
# expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0130)
})
test_that("Test TestGradientDescent",{
parameters <- TestGradientDescent(X = X, Y = Y)
# probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0135)
# Fail, test
# expect_equal(as.numeric(round(prob_new_student, digits = 4)), 0.0130)
})
View(data)
View(X)
View(Y)
Z[1]=2
length(Y)
B <- rep(0, 4)
B
B[2] <- 3
B
test_that("Test TestGradientDescent",{
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(X[i,2], X[i,3])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
})
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(X[i,2], X[i,3])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
X[i,2]
X[[i,2]]
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(X[[i,2]], X[[i,3]])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
X[[i,2]]
X[[i,3]]
Sigmoid(t(new_student) %*% parameters)
new_student
parameters
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(1, X[[i,2]], X[[i,3]])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = Z , reference = Y)
Z
Y
as.factor(Y)
confusionMatrix(data = Z , reference = as.factor(Y))
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y))
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(1, X[[i,2]], X[[i,3]])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
print(prob_new_student)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y))
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) {
new_student <- c(1, X[[i,2]], X[[i,3]])
prob_new_student <- Sigmoid(t(new_student) %*% parameters)
if (prob_new_student > 0.5) {
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y))
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) { #Se recorre el vector
new_student <- c(1, X[[i,2]], X[[i,3]]) #Se extraen las dos calificaciones
prob_new_student <- Sigmoid(t(new_student) %*% parameters) #Se calcula la probabilidad de aprobado del estudiante
if (prob_new_student > 0.7) { #Se establece como criterio de admisión (arbitrario) que solo alumnos con más de un 7 sean admitidos
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y))
parameters <- rep(0, ncol(X))
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
iterations = 1200
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
parameters_optimization
iteraciones <- seq(1,5000,50)
iteraciones <- seq(1,5000,50)
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
}
return(parameters)
}
iteraciones <- seq(1,5000,50)
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
}
return(parameters)
}
iteraciones <- seq(1,5000,50)
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
}
return(parameters)
}
TestCost(X,Y)
iteraciones <- seq(1,5000,10)
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
}
return(parameters)
}
TestCost(X,Y)
iteraciones <- seq(1,5000,10)
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
costes[iterations] <- convergence
}
return(costes)
}
TestCost(X,Y)
costes
iteraciones <- seq(1,5000,10)
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
costes[iterations] <- convergence
}
return(costes)
}
Prueba <- TestCost(X,Y)
Prueba
Prueba[1]
Prueba[10]
Prueba[11]
Prueba[21]
iteraciones <- 1:5000
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
costes[iterations] <- convergence
}
return(costes)
}
Prueba <- TestCost(X,Y)
Prueba
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
print(iterations)
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
costes[iterations] <- convergence
}
return(costes)
}
Prueba <- TestCost(X,Y)
plot(iteraciones, Prueba)
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
costes[iterations] <- convergence
}
return(costes)
}
Prueba <- TestCost(X,Y)
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
costes[iterations] <- convergence <- c(CostFunction(parameters, X, Y))
}
return(costes)
}
Prueba <- TestCost(X,Y)
plot(iteraciones, Prueba)
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
costes[iterations] <- convergence <- c(CostFunction(parameters, X, Y))
}
return(costes)
}
Prueba <- TestCost(X,Y)
plot(iteraciones, Prueba, "l")
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
costes[iterations] <- convergence <- c(CostFunction(parameters, X, Y))
}
return(costes)
}
Prueba <- TestCost(X,Y)
plot(iteraciones, Prueba, "l", main = "Evolución de la función de coste según el número de iteraciones", xlab = "Iteraciones", ylab = "Función de coste")
iteraciones <- 1:1500
costes <- rep(0, length(iteraciones))
TestCost <- function(X, Y) {
for (iterations in iteraciones) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
#set parameters
parameters <- parameters_optimization$par
# Check evolution
costes[iterations] <- convergence <- c(CostFunction(parameters, X, Y))
}
return(costes)
}
costs <- TestCost(X,Y)
plot(iteraciones, costs, "l", main = "Evolución de la función de coste según el número de iteraciones", xlab = "Iteraciones", ylab = "Función de coste")
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) { #Se recorre el vector
new_student <- c(1, X[[i,2]], X[[i,3]]) #Se extraen las dos calificaciones
prob_new_student <- Sigmoid(t(new_student) %*% parameters) #Se calcula la probabilidad de aprobado del estudiante
if (prob_new_student > 0.7) { #Se establece como criterio de admisión (arbitrario) que solo alumnos con más de un 7 sean admitidos
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y), dnn = c("Reales", "Predichos"))
parameters <- TestGradientDescent(X = X, Y = Y)
Z <- rep(0, length(Y))
for (i in 1:length(Y)) { #Se recorre el vector
new_student <- c(1, X[[i,2]], X[[i,3]]) #Se extraen las dos calificaciones
prob_new_student <- Sigmoid(t(new_student) %*% parameters) #Se calcula la probabilidad de aprobado del estudiante
if (prob_new_student > 0.7) { #Se establece como criterio de admisión (arbitrario) que solo alumnos con más de un 7 sean admitidos
Z[i] <- 1
}
}
library(caret)
confusionMatrix(data = as.factor(Z) , reference = as.factor(Y), dnn = c("Predicción", "Realidad"))

frecuencia <- read_csv("frecuencias_0619.csv")
frecuencia <- read.csv("frecuencias_0619.csv")
View(frecuencia)
severidad <- read.csv("cuantias_0619.csv")
View(severidad)
severidad <- read.csv("cuantias_0619.csv", dec = ",")
View(severidad)
severidad <- read.csv("cuantias_0619.csv", dec = ",")
severidad <- read.csv("cuantias_0619.csv", sep = "\n" dec = ",")
severidad <- read.csv("cuantias_0619.csv", sep = "\n", dec = ",")
View(severidad)
table(frecuencia)
length(frecuencia)
size(frecuencia)
shape(frecuencia)
View(frecuencia)
sum(frecuencia)
skewness(frecuencia)
library(moments)
skewness(frecuencia)
summary(frecuencia)
var(frecuencia)
kurtosis(frecuencia)
quantile(frecuencia,seq(0,1, 0.20))
quantile(frecuencia$x,seq(0,1, 0.20))
quantile(frecuencia$x, seq(0.9,1, 0.01))
hist(datos_agregados$siniestros, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Número Fraudes", ylab = "Frecuencia")
hist(frecuencia, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Número Fraudes", ylab = "Frecuencia")
hist(frecuencia$x, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Número Fraudes", ylab = "Frecuencia")
hist(frecuencia$x, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Personas que superan su esperanza de vida", ylab = "Frecuencia")
summary(severidad)
severidad
var(severidad)
table(severidad)
sum(severidad)
skewness(severidad)
kurtosis(severidad)
quantile(severidad$x,seq(0,1, 0.20))
quantile(severidad$x, seq(0.9,1, 0.01))
hist(severidad$x, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Personas que superan su esperanza de vida", ylab = "Frecuencia")
ggplot(severidad) + geom_density(aes(x), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
library(ggplot2)
ggplot(severidad) + geom_density(aes(x), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
ggplot(severidad) + geom_density(aes(x), fill = 'red') +
xlab('Severidad') + ylab('Densidad')
library(fitdistrplus)
(fpois <- fitdist(datos_agregados$siniestros, "pois"))
(fpois <- fitdist(frecuencia, "pois"))
(fpois <- fitdist(frecuencia$x, "pois"))
(fnbinom <- fitdist(frecuencia$x, "nbinom"))
gofstat(list(fpois, fnbinom), chisqbreaks=c(0:4, 9), discrete=TRUE,
fitnames=c("Poisson", "Binomial Negativa"))
(ajuste_frec <- gofstat(list(fpois, fnbinom), chisqbreaks = c(0:4, 9), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa")))
ajuste_frec$chisqpvalue
plot(fpois)
plot(fnbinom)
# Gamma
fgam <- fitdist(severidad, "gamma", lower = 0)
# Gamma
fgam <- fitdist(severidad$x, "gamma", lower = 0)
summary(fgam)
plot(fgam)
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 3))
library(fitdistrplus)
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 3))
# Pareto
fpar <- fitdist(severidad$x, "pareto")
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 0, scale = 0))
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 1))
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 0.5, scale = 1))
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 0.5, scale = 1.2))
# Pareto
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 100, scale = 15))
library(actuar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 3))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 0, scale = 0))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 1))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 1))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1.5, scale = 1))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 3))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2.2))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 5, scale = 4))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 5))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 6))
summary(fpar)
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 2, scale = 1))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2.5))
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2.2))
summary(fpar)
plot(fpar)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 2, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 1, shape2 = 1, scale = 1),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 1, shape2 = 2, scale = 1),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 1, shape2 = 1, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 0, shape2 = 1, scale = 2),
lower = c(0, 0, 0))
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 2, shape2 = 1, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 2, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 3, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 3, shape2 = 2, scale = 1),
lower = c(0, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 3, shape2 = 2, scale = 1),
lower = c(1, 0, 0))
summary(fburr)
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 3, shape2 = 2, scale = 1),
lower = c(1, 1, 1))
# Burr
fburr <- fitdist(severidad$x, "burr", method = 'mle', start = list(shape1 = 3, shape2 = 2, scale = 1),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Pruebas de bondad de ajuste
ajuste_sev <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr"))
# Pruebas de bondad de ajuste
(ajuste_sev <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr")))
FDD = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("black", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
FDD
comparación = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
comparacion
fpar <- fitdist(severidad$x, "pareto", start = list(shape = 1, scale = 2.2))
summary(fpar)
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
comparacion
plot(fnbinom)
comparacion_frec = cdfcomp(list(fpois, fnbinom), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "blue"),
fitlty = 2:5, legendtext = c("Poisson", "Binomial Neg"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
comparacion_frec
comparacion_frec = cdfcomp(list(fpois, fnbinom), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("green", "blue"),
fitlty = 2:5, legendtext = c("Poisson", "Binomial Neg"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
comparacion_frec
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
comparacion
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE, xlab = "Cuantía"
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste severidad", plotstyle = "ggplot")
comparacion
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE, xlab = "Cuantía",
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste severidad", plotstyle = "ggplot")
comparacion
View(frecuencia)
max(frecuencia$x)
frecuencia$x[1:12]
max(frecuencia$x)[1:12]
max(frecuencia$x[1:12])
max(frecuencia$x[1:12])
max(frecuencia$x[13:24])
max(frecuencia$x[25:36])
max(frecuencia$x[37:48])
max(frecuencia$x[49:60])
139 + 139 + 84 + 113 + 147 + 67
689/5
umbral <- 137
exceso <- severidad[severidad > umbral]
exceso
max(severidad$x[1:12])
max(severidad$x[13:24])
max(severidad$x[25:36])
max(severidad$x[37:48])
max(severidad$x[49:60])
Z <- a + b +c +d +e
a <- max(severidad$x[1:12])
b <- max(severidad$x[13:24])
c <- max(severidad$x[25:36])
d <- max(severidad$x[37:48])
e <- max(severidad$x[49:60])
Z <- a + b +c +d +e
Z/5
max(severidad$x[1:12])
max(severidad$x[13:24])
max(severidad$x[25:36])
max(severidad$x[37:48])
max(severidad$x[49:60])
umbral <- 1180000
exceso <- severidad[severidad > umbral]
exceso
exceso <- severidad[severidad$x > umbral]
exceso
exceso <- severidad$x[severidad$x > umbral]
exceso
umbral <- 118000
exceso <- severidad$x[severidad$x > umbral]
exceso
exceso <- severidad[severidad > umbral]
exceso
pasan <- length(exceso)
pasan
prob <- 1 - base::rank(exceso)/(pasan + 1)  #rank ofrece el n de orden
prob
# Graficar la ditribución empricial y la de superviencia ajustada
plot(exceso, prob, log = "xy", xlab = "Excesos",
ylab = "Probabilidades", ylim = c(0.01, 1))
alfa <- -cov(log(exceso), log(prob)) / var(log(exceso))
alfa
x = seq(u, max(exceso), length = 100) #divide de u a max() 100 interv.
x = seq(u, max(exceso), length = 100) #divide de u a max() 100 interv.
x = seq(umbral, max(exceso), length = 100) #divide de u a max() 100 interv.
y = (x / umbral)^(-alfa)
lines(x, y)
prob <- rank(exceso) / (pasan + 1)
plot(exceso, prob, log = "x", xlab = "Excesos", ylab = "Probabilidades de no excesos")
y = 1 - (x / umbral)^(-alfa)
lines(x, y)
#Distribucion valores extremos generalizados (GEV)
nllik.gev <- function(par, data){
mu <- par[1]
sigma <- par[2]
xi <- par[3]
if ((sigma <= 0) | (xi <= -1))
return(1e6)
n <- length(data)
if (xi == 0)
n * log(sigma) + sum((data - mu) / sigma) +
sum(exp(-(data - mu) / sigma))
else {
if (any((1 + xi * (data - mu) / sigma) <= 0))
return(1e6)
n * log(sigma) + (1 + 1 / xi) *
sum(log(1 + xi * (data - mu) / sigma)) +
sum((1 + xi * (data - mu) / sigma)^(-1/xi))
}
}
# GEV
sigma.start <- sqrt(6) * sd(exceso) / pi
mu.start <- mean(exceso) + digamma(1) * sigma.start
fit.gev <- nlm(nllik.gev, c(mu.start, sigma.start, 0),
hessian = TRUE, data = exceso)
fit.gev
.
fit.gev$estimate
sqrt(diag(solve(fit.gev$hessian)))
tau <- par[1]
nllik.gp <- function(par, u, data){
tau <- par[1]
xi <- par[2]
if ((tau <= 0) | (xi < -1))
return(1e6)
m <- length(data)
if (xi == 0)
m * log(tau) + sum(data - u) / tau
else {
if (any((1 + xi * (data - u) / tau) <= 0))
return(1e6)
m * log(tau) + (1 + 1 / xi) *
sum(log(1 + xi * (data - u) / tau))
}
}
# Obtención de los parámetros PARETO
tau.start <- mean(exceso) - umbral
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
data = exceso)
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = umbral, hessian = TRUE,
data = exceso)
fit.gp
fit.gp$estimate
sqrt(diag(solve(fit.gp$hessian)))
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
qqgpd(severidad, umbral, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
ppgpd(severidad, umbral, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
fnbinom
fnbinom$estimate
fnbinom$estimate[0]
fnbinom$estimate[1]
perdida_agregada <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = fnbinom$estimate[1], mu = fnbinom$estimate[2])),
model.sev = expression(y = rburr(shape1 = fburr$estimate[1], shape2 = fburr$estimate[2],
scale = fburr$estimate[3])),
nb.simul = 1000)
perdida_agregada
plot(perdida_agregada)
# Deteminacion del Value at Risk al 90%
quantile(perdida_agregada, 0.9)
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE, xlab = "Cuantía",
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "yellow", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste severidad", plotstyle = "ggplot")
comparacion
comparacion = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE, xlab = "Cuantía",
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("pink", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste severidad", plotstyle = "ggplot")
comparacion
perdida_agregada <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = fnbinom$estimate[1], mu = fnbinom$estimate[2])),
model.sev = expression(y = rburr(shape1 = fburr$estimate[1], shape2 = fburr$estimate[2],
scale = fburr$estimate[3])),
nb.simul = 10000)
perdida_agregada
plot(perdida_agregada)
# Deteminacion del Value at Risk al 90%
quantile(perdida_agregada, 0.9)
perdida_agregada <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = fnbinom$estimate[1], mu = fnbinom$estimate[2])),
model.sev = expression(y = rburr(shape1 = fburr$estimate[1], shape2 = fburr$estimate[2],
scale = fburr$estimate[3])),
nb.simul = 1000)
perdida_agregada
plot(perdida_agregada)
exceso

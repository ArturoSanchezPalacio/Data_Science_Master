lines(seq(0,100), Fs.n(seq(0,100)), lty =3)
plot(seq(0,100), agregada(seq(0,100)), type = "l", xlim = c(0, 100))
lines(seq(0,100), Fs.n(seq(0,100)), lty =3)
sla(opriskmodel,.95)
library(OpVaR)
install.packages("OpVaR")
data(lossdat)
library(OpVaR)
data(lossdat)
View(lossdat)
sla(danishuni, 0.95)
source('~/Documents/CUNEF/Riesgo operacional/Practica Final/codigo_informe.R', echo=TRUE)
parsev$estimate
ajuste_burr$estimate
severidad <- rburr(n = simulaciones, shape1 = 0.1, shape2 = 14.44, scale = 1.08)
burr.sev <- data.frame(Severity = severidad,
Distribution = rep("Burr", each = simulaciones))
ggplot(burr.sev, aes(x = Severity, fill = Distribution)) +
geom_density(alpha = 0.3)
parfreq
frecuencia <- rpois(simulaciones, parfreq)
poisson.freq <- data.frame(Frequency = frecuencia,
Distribution = rep("Poisson", each = simulaciones))
ggplot(poisson.freq, aes(x = frequency,
fill = Distribution)) + geom_density(alpha = 0.3)
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
perdida_agregada <- rpois(simulaciones, severidad * parfreq)
perdidas_df <- data.frame(Loss = perdida_agregada, Distribution = rep("Potential Loss", each = simulaciones))
(VaR.loss.rf <- quantile(perdidas_df$Loss, 0.95))
(VaR.loss.rf <- quantile(perdidas_df$Loss, 0.05))
View(perdidas_df)
summary(perdidas_df$Loss)
View(perdidas_df)
severidad * parfreq
(ES.loss.rf <- mean(perdidas_df$Loss[perdidas_df$Loss >
VaR.loss.rf]))
simulaciones <- 2167
severidad <- rburr(n = simulaciones, shape1 = 0.1, shape2 = 14.44, scale = 1.08)
burr.sev <- data.frame(Severity = severidad,
Distribution = rep("Burr", each = simulaciones))
ggplot(burr.sev, aes(x = Severity, fill = Distribution)) +
geom_density(alpha = 0.3)
frecuencia <- rpois(simulaciones, parfreq)
poisson.freq <- data.frame(Frequency = frecuencia,
Distribution = rep("Poisson", each = simulaciones))
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
perdida_agregada <- rpois(simulaciones, severidad * parfreq)
perdidas_df <- data.frame(Loss = perdida_agregada, Distribution = rep("Potential Loss", each = simulaciones))
(VaR.loss.rf <- quantile(perdidas_df$Loss, 0.05))
(VaR.loss.rf <- quantile(perdidas_df$Loss, 0.95))
(ES.loss.rf <- mean(perdidas_df$Loss[perdidas_df$Loss >
VaR.loss.rf]))
perdidas_df$Loss
severidad*frecuencia
summary(severidad*frecuencia)
quantile(severidad*frecuencia, 0.95)
quantile(severidad*frecuencia, 0.99)
View(danishuni)
ggplot(burr.sev, aes(x = Severity, fill = Distribution)) +
geom_density(alpha = 0.3)
ggplot(burr.sev, aes(x = Severity, fill = Distribution)) +
geom_density(alpha = 0.3)
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
perdidas_df <- data.frame(Loss = perdida_agregada, Distribution = rep("Potential Loss", each = simulaciones))
View(perdidas_df)
ggplot(perdidas_df$Loss)
ggplot(data = perdidas_df, aes(x = Loss))
ggplot(data = perdidas_df, aes(x = Loss)) + geom_density(alpha = 0.3)
ggplot(data = perdidas_df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.3)
(RM.danish <- RiskMeasures(perdidas_df$Loss,
c(0.99, 0.995)))
install.packages("QRM")
library(QRM)
(RM.danish <- RiskMeasures(perdidas_df$Loss,
c(0.99, 0.995)))
View(perdidas_df)
cuantil <- 0.95
u <- quantile(danish, cuantil,names = FALSE)
fit.danish <- fit.GPD(danish, threshold = u)
(xi.hat.danish <- fit.danish$par.ses[["xi"]])
(beta.hat.danish <- fit.danish$par.ses[["beta"]])
loss.excess <- danish[danish > u] - u
(VaR.gpd <- u + (beta.hat.danish/xi.hat.danish) *
(((1 - cuantil)/n.relative.excess)^(-xi.hat.danish) -
1))
n.relative.excess <- length(loss.excess)/length(danish)
(VaR.gpd <- u + (beta.hat.danish/xi.hat.danish) *
(((1 - cuantil)/n.relative.excess)^(-xi.hat.danish) -
1))
(ES.gpd <- (VaR.gpd + beta.hat.danish -
xi.hat.danish * u)/(1 - xi.hat.danish))
danish
(RM.danish <- RiskMeasures(perdidas_df$Loss,
c(0.99, 0.995)))
(RM.danish <- RiskMeasures(danish,
c(0.99, 0.995)))
(RM.danish <- RiskMeasures(severidad*frecuencia,
c(0.99, 0.995)))
quantile(severidad*frecuencia, 0.99)
source('~/Documents/CUNEF/Riesgo operacional/Practica Final/codigo_informe.R', echo=TRUE)
summary(datos_agregados)
var(datos_agregados)
var(datos_agregados$Loss)
quantile(datos_agregados$Loss,seq(0,1, 0.20))
quantile(datos_agregados$Loss,seq(0.9,1, 0.01))
hist(datos_agregados$Loss, pch = 5, breaks = 50, prob = TRUE, main = "Pérdidas",
xlab = " X", ylab = "Valor", xlim = c(0,40))
View(datos_agregados)
hist(datos_agregados$Loss, pch = 5, breaks = 50, prob = TRUE, main = "Pérdidas",
xlab = " X", ylab = "Valor")
hist(datos_agregados$Loss, pch = 5, breaks = 50, prob = TRUE, main = "Pérdidas",
xlab = " Importe", ylab = "Frecuencia")
quantile(datos_agregados$Loss,seq(0,1, 0.20))
summary(datos_agregados)
mean(perdidas)
median(perdidas)
quantile(perdidas,seq(0.9,1, 0.01))
summary(perdidas)
fitdistr(frecuencia$siniestros, "negbin")
frecuencia <- siniestros_dia[,2]
fitdistr(frecuencia$siniestros,"poisson")
fitdistr(frecuencia$siniestros, "negbin")
fitdistr(frecuencia$siniestros, "binomial")
fitdistr(frecuencia$siniestros, "negative binomial")
(ajuste_poisson <- fitdistr(frecuencia$siniestros,"poisson"))
(ajuste_bin <- fitdistr(frecuencia$siniestros, "negative binomial"))
cdfcomp(list(ajuste_poisson, ajuste_bin), xlogscale = TRUE, datapch = ".",
datacol = "red", fitcol = "black", fitlty = 2:5, legendtext = c("Poisson","Binomial Negativa"),
main = "Comparación de ajustes")
(ajuste_poisson <- fitdist(frecuencia$siniestros,"poisson"))
dpoisson <- dpois()
function depoisson() <- depois()
(ajuste_poisson <- fitdistr(frecuencia$siniestros,"poisson"))
(ajuste_bin <- fitdistr(frecuencia$siniestros, "negative binomial"))
plot(frecuencia)
plot(frecuencia, 'l')
hist(frecuencia)
frecuencia
hist(frecuencia$siniestros)
hist(ajuste_poisson)
ajuste_poisson
ggplot(poisson.freq, aes(x = frequency,
fill = Distribution)) + geom_density(alpha = 0.3)
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
(ajuste_poisson <- fitdistr(frecuencia$siniestros,"poisson"))
(ajuste_bin <- fitdistr(frecuencia$siniestros, "negative binomial"))
(ajuste_poisson <- fitdist(frecuencia$siniestros, "pois"))
(ajuste_bin <- fitdist(frecuencia$siniestros, "nbinom"))
gofstat(list(ajuste_poisson, ajuste_bin), chisqbreaks=c(0:4, 9), discrete=TRUE,
fitnames=c("Poisson", "Binomial Negativa"))
perdidas <- danishuni$Loss
ajuste_gamma <- fitdist(perdidas, "gamma", lower = 0)
ajuste_pareto <- fitdist(perdidas, "pareto", start = list(shape = 2, scale = 2), lower = 0)
dmixgampar <- function(x, prob, nu, lambda, alpha, theta)
prob*dgamma(x, nu, lambda) + (1 - prob)*dpareto(x, alpha, theta)
pmixgampar <- function(q, prob, nu, lambda, alpha, theta)
prob*pgamma(q, nu, lambda) + (1 - prob)*ppareto(q, alpha, theta)
ajuste_mixtura <- fitdist(perdidas, "mixgampar", start = list(prob = 1/2, nu = 1, lambda = 1, alpha = 2, theta = 2), lower = 0)
cbind(SINGLE = c(NA, ajuste_gamma$estimate, ajuste_pareto$estimate), MIXTURE = ajuste_mixtura$estimate)
ajuste_burr <- fitdist(perdidas, "burr", start = list(shape1 = 2, shape2 = 2, scale = 2), lower = c(0.1,1/2, 0))
ajuste_burr$estimate
cdfcomp(list(ajuste_gamma, ajuste_pareto, ajuste_mixtura, ajuste_burr), xlogscale = TRUE, datapch = ".",
datacol = "red", fitcol = "black", fitlty = 2:5, legendtext = c("gamma","Pareto","Par-gam","Burr"),
main = "Comparación de ajustes")
ajuste_burr$estimate
ppcomp(list(ajuste_burr), xlogscale = TRUE, ylogscale = TRUE, fitcol = "black", main = "PP-plot Danishuni",
legendtext = c("Burr"), fitpch = 1)
qqcomp(list(ajuste_burr), xlogscale = TRUE, ylogscale = TRUE, fitcol = "black", main = "QQ-plot Danishuni",
legendtext = c("Burr"), fitpch = 1)
ggplot(burr.sev, aes(x = Severity, fill = Distribution)) +
geom_density(alpha = 0.3)
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution, xlim = c(0,10))) + geom_density(alpha = 0.3)
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
poisson.freq <- data.frame(Frequency = frecuencia,
Distribution = rep("Poisson", each = simulaciones))
frecuencia <- rpois(simulaciones, parfreq)
poisson.freq <- data.frame(Frequency = frecuencia,
Distribution = rep("Poisson", each = simulaciones))
ggplot(poisson.freq, aes(x = frecuencia,
fill = Distribution)) + geom_density(alpha = 0.3)
perdida_agregada <- rpois(simulaciones, severidad * parfreq)
perdidas_df <- data.frame(Loss = perdida_agregada, Distribution = rep("Potential Loss", each = simulaciones))
ggplot(data = perdidas_df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.3)
cuantil <- 0.99
u <- quantile(danish, cuantil,names = FALSE)
fit.danish <- fit.GPD(danish, threshold = u)
(xi.hat.danish <- fit.danish$par.ses[["xi"]])
(beta.hat.danish <- fit.danish$par.ses[["beta"]])
loss.excess <- danish[danish > u] - u
n.relative.excess <- length(loss.excess)/length(danish)
(VaR.gpd <- u + (beta.hat.danish/xi.hat.danish) *
(((1 - cuantil)/n.relative.excess)^(-xi.hat.danish) -
1))
(ES.gpd <- (VaR.gpd + beta.hat.danish -
xi.hat.danish * u)/(1 - xi.hat.danish))
gofstat(list(ajuste_poisson, ajuste_bin), chisqbreaks = c(0:4, 9), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
parsev
plot(seq(0,100), Fs.n(seq(0,100)), type = "l", xlim = c(0, 100))
source('~/Documents/CUNEF/Riesgo operacional/Practica Final/codigo_informe.R', echo=TRUE)
library(moments)
library(actuar)
library(fitdistrplus)
library(ggplot2)
library(MASS)
library(CASdatasets)
library(car)
library(dplyr)
library(lubridate)
library(OpVaR)
library(QRM)
data(danishuni)
head(danishuni)
library(moments)
library(actuar)
library(fitdistrplus)
library(ggplot2)
library(MASS)
library(CASdatasets)
library(car)
library(dplyr)
library(lubridate)
library(OpVaR)
library(QRM)
datos_agregados <- danishuni %>% group_by(meses = floor_date(danishuni$Date, "month")) %>% summarize(siniestros = n(), Loss = sum(Loss))
datos_agregados <- danishuni %>% group_by(meses = floor_date(danishuni$Date, "week")) %>% summarize(siniestros = n(), Loss = sum(Loss))
datos_agregados <- danishuni %>% group_by(semanas = floor_date(danishuni$Date, "week")) %>% summarize(siniestros = n(), Loss = sum(Loss))
View(datos_agregados)
summary(datos_agregados)
View(danishuni)
View(datos_agregados)
View(datos_agregados)
table(x$Fraudes)
table(datos_agregados$siniestros)
lengh(datos_agregados$siniestros)
length(datos_agregados$siniestros)
sum(datos_agregados$siniestros)
skewness(datos_agregados$siniestros)
skewness(datos_agregados$siniestros)
skewness(datos_agregados$Loss)
kurtosis(datos_agregados$siniestros)
quantile(datos_agregados$siniestros,seq(0,1, 0.20))
quantile(datos_agregados$siniestros,seq(0,1, 0.20))
quantile(datos_agregados$siniestros, seq(0.9,1, 0.01))
View(datos_agregados)
quantile(datos_agregados$siniestros,seq(0,1, 0.20))
quantile(datos_agregados$siniestros, seq(0.9,1, 0.01))
hist(datos_agregados$siniestros, pch = 20, breaks = 25, prob = TRUE, main = "HISTOGRAMA",
xlab = "Número Fraudes", ylab = "Frecuencia")
hist(datos_agregados$siniestros, pch = 20, breaks = 25, prob = TRUE, main = "Histograma frecuencia",
xlab = "Número Fraudes", ylab = "Frecuencia")
(fpois <- fitdist(x$Fraudes, "pois"))
(fpois <- fitdist(datos_agregados$siniestros, "pois"))
plot(fpois)
(fnbinom <- fitdist(datos_agregados$siniestros, "nbinom"))
gofstat(list(fpois, fnbinom), chisqbreaks=c(0:4, 9), discrete=TRUE,
fitnames=c("Poisson", "Binomial Negativa"))
# Cargar librerias
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
# Cargar los datos
data(danishuni)
# Tratamiento de los datos (agrupacion por mes)
# Objetivo obtener la frecuencia en las perdidas mensuales
df_perdidas <- danishuni %>%
group_by(floor_date(Date, 'months')) %>%
summarise(Freq = length(Loss), PerdidasM = sum(Loss))
# A) Frecuencia de las perdidas mensuales
frecuencia <- df_perdidas$Freq
# Alta concentracion de valores en el centro de la distribucion
table(frecuencia)
# La media y la mediana se encuentran un poco lejanas
summary(frecuencia)
# Existe una alta varianza en la frecuencia mensual
var(frecuencia)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(frecuencia)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(frecuencia)
# Graficar la distribucion frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(Freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
dev.off()
# Cuantiles
quantile(frecuencia, probs = c(0.05, 0.95))
quantile(frecuencia, seq(0,1, 0.20))
quantile(frecuencia, seq(0.9,1, 0.01))
# B) Severidad de las perdidas mensuales
severidad <- df_perdidas$PerdidasM
# Gran cantidad de valores en el centro de la distribución
table(severidad)
# La media y la mediana se encuentran lejanas
summary(severidad)
# Existe una alta varianza en la severidad mensual
var(severidad)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(severidad)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(severidad)
# Graficar la distribucion severidad
png("./imagenes/distribucion severidad.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(PerdidasM), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
dev.off()
# Cuantiles
quantile(severidad, probs = c(0.05, 0.95))
quantile(severidad, seq(0,1, 0.20))
quantile(severidad, seq(0.9,1, 0.01))
# Distribucion Poisson, metodo máxima verosimilitud
fpois = fitdist(frecuencia, "pois", method = 'mle')
summary(fpois)
plot(fpois)
# Distribucion  Binomial Negativa, metodo máxima verosimilitud
fnbinom = fitdist(frecuencia, "nbinom", 'mle')
summary(fnbinom)
plot(fnbinom)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
cdfcomp(list(fpois, fnbinom))
dev.off()
# Pruebas de bondad de ajuste
ba_discreta <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
# Estadítico de bondad de ajuste: Chi test(pvalue)
ba_discreta$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
# Criterio de bondad de ajuste (AIC y BIC)
ba_discreta$aic; ba_discreta$bic #se escoge la Bino. Neg por menor aic y bic
# Resultado: La distribucion de frecuencia se ajusta mejor
# con una binomial negativa
size = fnbinom$estimate[1]
mu = fnbinom$estimate[2]
# GAMMA
fgam <- fitdist(severidad, "gamma", lower = 0)
summary(fgam)
plot(fgam)
# PARETO
fpar <- fitdist(severidad, "pareto", start = list(shape = 2, scale = 3))
summary(fpar)
plot(fpar)
# BURR
fburr <- fitdist(severidad, "burr", method = 'mle', start = list(shape1 = 2, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Pruebas de bondad de ajuste
ba_continua <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr"))
# Estadítico de bondad de ajuste: Kolmogorov-Smirnov statistic
ba_continua$ks
ba_continua$kstest
# Criterio de bondad de ajuste
ba_continua$aic; ba_continua$bic
# Comprobacion grafica entre las distribuciones continuas
FDD = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("black", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
# La distribución se ajusta mejor a burr
png("./imagenes/ajuste_severidad.png", width = 800, height = 500)
FDD
dev.off()
# Resultado: La distribucion de severidad se ajusta mejor
# con una distribución burr
shape1 = fburr$estimate[1]
shape2 = fburr$estimate[2]
scale1 = fburr$estimate[3]
# Obtener la pérdida maxima por año
danishuni %>% group_by(floor_date(Date, 'year')) %>%
summarise(con = max(Loss))
# Exceso sobre umbral. Extrac. de Valores que exceden un umbral (POT)
u <- 54
View(df_perdidas)
exceden_umbral <- severidad[severidad > u]
exceden_umbral
#Visualizacion de las colas
n.u <- length(exceden_umbral) #n de casos que superan el umbral
surv.prob <- 1 - base::rank(exceden_umbral)/(n.u + 1)  #rank ofrece el n de orden
surv.prob
# Graficar la ditribución empricial y la de superviencia ajustada
plot(exceden_umbral, surv.prob, log = "xy", xlab = "Excesos",
ylab = "Probabilidades", ylim = c(0.01, 1))
#Determinamos los parámetros necesarios
alpha <- -cov(log(exceden_umbral), log(surv.prob)) / var(log(exceden_umbral))
alpha
x = seq(u, max(exceden_umbral), length = 100) #divide de u a max() 100 interv.
y = (x / u)^(-alpha)
lines(x, y)
#Función de distribución acumulada
prob <- rank(exceden_umbral) / (n.u + 1)
plot(exceden_umbral, prob, log = "x", xlab = "Excesos", ylab = "Probabilidades de no excesos")
y = 1 - (x / u)^(-alpha)
lines(x, y)
#Distribucion valores extremos generalizados (GEV)
nllik.gev <- function(par, data){
mu <- par[1]
sigma <- par[2]
xi <- par[3]
if ((sigma <= 0) | (xi <= -1))
return(1e6)
n <- length(data)
if (xi == 0)
n * log(sigma) + sum((data - mu) / sigma) +
sum(exp(-(data - mu) / sigma))
else {
if (any((1 + xi * (data - mu) / sigma) <= 0))
return(1e6)
n * log(sigma) + (1 + 1 / xi) *
sum(log(1 + xi * (data - mu) / sigma)) +
sum((1 + xi * (data - mu) / sigma)^(-1/xi))
}
}
# GEV
sigma.start <- sqrt(6) * sd(exceden_umbral) / pi
mu.start <- mean(exceden_umbral) + digamma(1) * sigma.start
fit.gev <- nlm(nllik.gev, c(mu.start, sigma.start, 0),
hessian = TRUE, data = exceden_umbral)
fit.gev
# Parametros: posición, escala y forma
fit.gev$estimate
sqrt(diag(solve(fit.gev$hessian)))
# B) Modelo Poisson-Generalizada de Pareto
nllik.gp <- function(par, u, data){
tau <- par[1]
xi <- par[2]
if ((tau <= 0) | (xi < -1))
return(1e6)
m <- length(data)
if (xi == 0)
m * log(tau) + sum(data - u) / tau
else {
if (any((1 + xi * (data - u) / tau) <= 0))
return(1e6)
m * log(tau) + (1 + 1 / xi) *
sum(log(1 + xi * (data - u) / tau))
}
}
# Obtención de los parámetros PARETO
tau.start <- mean(exceden_umbral) - u #valores medios entre el umbral y los excesos
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
data = exceden_umbral)
fit.gp
fit.gp$estimate
sqrt(diag(solve(fit.gp$hessian)))
#Intervalo de confianza del valor máximo del indice de cola al 95%
prof.nllik.gp <- function(par,xi, u, data)
nllik.gp(c(par,xi), u, data)
prof.fit.gp <- function(x)
-nlm(prof.nllik.gp, tau.start, xi = x, u = u, hessian = TRUE,
data = exceden_umbral)$minimum #ojo cambiar base de datos
vxi = seq(0,1.8,by = .025)
prof.lik <- Vectorize(prof.fit.gp)(vxi)
# Grafico intervalos de confianza
png("./imagenes/perfil_verosimilutd.png", width = 800, height = 500)
plot(vxi, prof.lik, type = "l", xlab = expression(xi),
ylab = "Profile log-likelihood")
opt <- optimize(f = prof.fit.gp, interval = c(0,3), maximum = TRUE)
opt
up <- opt$objective
abline(h = up, lty = 2)
abline(h = up - qchisq(p = 0.95, df = 1), col = "grey")
I <- which(prof.lik >= up - qchisq(p = 0.95, df = 1))
lines(vxi[I], rep(up - qchisq(p = 0.95, df = 1), length(I)),
lwd = 5, col = "grey")
abline(v = range(vxi[I]), col = "grey", lty = 2)
abline(v = opt$maximum, col = "grey")
dev.off()
# A) Q-Q Plot para la Dist. Generalizada de Pareto (DGP)
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
png("./imagenes/qq_plot.png", width = 800, height = 500)
qqgpd(severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
dev.off()
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
png("./imagenes/pp_plot.png", width = 800, height = 500)
ppgpd(severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
dev.off()
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
